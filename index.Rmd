---
title: "Machine_Learning - Exercise_Activity_Prediction"
author: "Willianto Asalim"
date: "18/07/2020"
output: html_document
---

##### The platform specification used:
Spec    | Description
------- | -----------------------
OS      | Windows 10 Pro - 64 bit
CPU     | AMD Ryzen 5 - 3400G
RAM     | 16GB DDR4 3000MHz
Storage | 500GB SSD - M.2 NVMe (PCIe) 
Tool    | RStudio


```{r LoadPackages, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr) ##Load Knitr package
library(ggplot2) ##Plotting and data
library(caret) ##Load package for ML
library(dplyr) ##Data transformation package
library(rattle) ##Fancy DT plot
library(SparseM) ##SVM predictor
```

```{r setoptions, echo=FALSE}
## Setting Global Option where echo = true so that someone will be able to read the code and results.
knitr::opts_chunk$set(echo = TRUE, results = "hold", tidy = TRUE)
```

## Human Activity Recognition

### 1. Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

Six male participants aged between 20 and 28 years old with little weight lifting experience were asked to perform one set of 10 repetitions of the Unilateral Dumbbell (1.25kg) Biceps Curl in five different fashions: 
Class A - exactly according to the specification , 
Class B - throwing the elbows to the front, 
Class C - lifting the dumbbell only halfway, 
Class D - lowering the dumbbell only halfway, 
Class E - throwing the hips to the front.

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate.

More information is available from the website here: [Human Activity Recognition info](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. A report describing how to build the model, how to use cross validation, what are the expected out of sample error is, and why the choices are being made.

Steps Taken

1.Exploratory Data Analysis - Load and tidy data. Remove columns with little/no data.

2.Slice data from training data for cross validation checking

3.Predicting using Ensemble Learning Stacking method - combining 3 predictors namely Random Forrest, Gradient Boosted Model and Linear Discriminant Analysis.

4.Fine tune model through combinations of above methods, reduction of input variables or similar. The fine tuning will take into account accuracy first and speed of analysis second.


### 2. Exploratory Data Analysis - Load and Tidy/Clean Data
First of all we need to download the train and test data set from the link provided
```{r DownloadData}
## Set the URL and file name for training dataset supplied
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainFile <- "./data/ML-Train.csv"

## Set the URL and file name for testing dataset supplied
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testFile <- "./data/ML-Test.csv"

if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(trainUrl, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
  download.file(testUrl, destfile=testFile, method="curl")
}
```

Secondly after we have have download the files we need to read the CSV files for training and testing dataset.
```{r ReadData}
##Various indicators of missing data (i.e., “NA”, “#DIV/0!” and “”) are all set to NA so they can be processed
trainRaw <- read.csv(trainFile, na.strings=c("NA","#DIV/0!",""))
testRaw <- read.csv(testFile, na.strings=c("NA","#DIV/0!",""))

##dim(trainRaw) ##Observations and variables of raw train dataset
##dim(testRaw) ##Observations and variables of raw test dataset
##str(trainRaw) ##Structure of the raw train dataset 
##str(testRaw) ##Structure of the raw test dataset

table(trainRaw$classe) ##Tabulation of raw train data with the variable (Classe) and its frequency
sum(complete.cases(trainRaw)) ##Sum of complete rows of a data frame
```

The raw train data contains `r nrow(trainRaw)` observations and `r ncol(trainRaw)` variables. Whereas the test raw data contains `r nrow(testRaw)` observations and `r ncol(testRaw)` variables.  
From the observation above we can see that there is no complete data set (without missing values). Therefore we need to remove columns with NA values

```{r RemoveNA}
##Remove colums with NAs
trainClean1 <- trainRaw[, colSums(is.na(trainRaw)) == 0] 
testClean1 <- testRaw[, colSums(is.na(testRaw)) == 0] 

##str(trainClean1) ##Structure of the raw train dataset after removing NA values
##str(testClean1) ##Structure of the raw test dataset after removing NA values
##dim(trainClean1) ##Observations and variables of raw train dataset after removing NA values
##dim(testClean1) ##Observations and variables of raw test dataset after removing NA values

table(trainClean1$classe) ##Tabulation of train data with the variable (Classe) and its frequency
sum(complete.cases(trainClean1)) ##Sum of complete rows of a data frame
```  

The complete cases (without missing values) is now complete at 19622 rows while maintaining the same amount frequency in the variable "Classe".
Then we proceed with removing unneccassary columns data such as X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window and num_window.

```{r RemoveColumns}
##Remove the first 7 columns as they are not necessary for modeling purpose
trainFinal <- trainClean1[, -c(1:7)]
testFinal <- testClean1[, -c(1:7)]

##str(trainFinal) ##Structure of the final train dataset
##str(testFinal) ##Structure of the final test dataset

dim(trainFinal) ##Observations and variables of final train dataset
dim(testFinal)  ##Observations and variables of final test dataset
```

**The final clean train data contains `r nrow(trainFinal)` observations and `r ncol(trainFinal)` variables. Whereas the test raw data contains `r nrow(testFinal)` observations and `r ncol(testFinal)` variables. We have removed a total of 107 variables from the raw data to the final data of both training and test datasets.**

### 3. Slice data from training data for cross validation checking

The training data is divided into two parts. This first is the training set with 70% of the original training data and it will be used to train the model. The second is a validation set used to validate model performance.

```{r SliceData}
set.seed(12345) # For reproducibile purpose

#Partition rows into training-70% and validation-30%
inTrain <- createDataPartition(trainFinal$classe, p=0.70, list=FALSE)
trainData <- trainFinal[inTrain, ]
validateData <- trainFinal[-inTrain,]

##dim(trainData) ##Observations and variables of final partitioned train dataset
##dim(validateData) ##Observations and variables of final partitioned validation dataset
```
The partitioned train data contains `r nrow(trainData)` observations and `r ncol(trainData)` variables. Whereas the validation data contains `r nrow(validateData)` observations and `r ncol(validateData)` variables.

### 4. Building Model
We will be building three different models namely Random Forrest, Gradient Boosted Model and Linear Discriminant Analysis.

```{r ModelFit, cache=TRUE}
##Random Forest Model Fit
modRf <- train(classe ~., method="rf", data=trainData, trControl=trainControl(method="cv"),number=3)

##Gradient Boosted Model Fit
ModGbm <- train(classe~., method="gbm", data=trainData, verbose=FALSE)

##Linear Discriminant Analysis Model Fit
modLda <- train(classe~., method="lda", data=trainData)
```

```{r PredictionCrossValidation, cache=TRUE}
predRf <- predict(modRf, newdata=validateData)

predGbm <- predict(ModGbm, newdata=validateData)

predLda <- predict(modLda, newdata=validateData)

```


```{r combinedPredictors, cache=True}
predDF <- data.frame(predRf, predGbm, predLda, classe=validateData$classe)
combModFit <- train(classe~., method="rf", data=predDF)
combPred <- predict(combModFit, data=predDF)

##combModFit2 <- train(classe~., method="gbm", data=predDF)
##combPred2 <- predict(combModFit2, data=predDF)
```

```{r AccuracyTest}
##modRf
confusionMatrix(predRf, as.factor(validateData$classe))$overall['Accuracy']

##ModGbm
confusionMatrix(predGbm, as.factor(validateData$classe))$overall['Accuracy']

##modLda
confusionMatrix(predLda, as.factor(validateData$classe))$overall['Accuracy']

confusionMatrix(combPred, as.factor(validateData$classe))$overall['Accuracy']

```


### 5. Predicting Test Result

```{r PredictTest, cache=TRUE}
predTest <- predict(modRf, newdata=testFinal)
predTest

```
